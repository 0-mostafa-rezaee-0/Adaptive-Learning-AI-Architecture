\documentclass[aspectratio=169]{beamer}

% Theme and color scheme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{adjustbox}

% Custom colors
\definecolor{myblue}{RGB}{0,102,204}
\definecolor{myred}{RGB}{204,0,0}
\definecolor{mygreen}{RGB}{0,153,0}
\definecolor{mypurple}{RGB}{128,0,128}
\definecolor{myorange}{RGB}{255,140,0}

% Title page information
\title{Information-Theoretic Analysis of Diffusion Model Dynamics: Optimal Scheduling, Sampling Bounds, and Conditional Generation}
\author{Iman Khazrak, Mostafa M. Rezaee, Mohammadhossein Homaei, Robert C. Green II}
\institute{Bowling Green State University \& University of Extremadura}
\date{\today}

\begin{document}

% Title slide
\begin{frame}
\titlepage
\end{frame}

% Outline
\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Introduction and Motivation}

\begin{frame}
\frametitle{Diffusion Models: Success and Challenges}
\begin{columns}
\begin{column}{0.6\textwidth}
\textbf{Recent Success:}
\begin{itemize}
\item State-of-the-art image generation quality
\item Text-to-image synthesis (DALL-E, Midjourney)
\item Superior mode coverage vs. GANs
\item FID scores: 1.97-3.17 on CIFAR-10
\end{itemize}

\vspace{0.5cm}
\textbf{Current Challenges:}
\begin{itemize}
\item \textcolor{myred}{Heuristic design choices}
\item \textcolor{myred}{No theoretical understanding}
\item \textcolor{myred}{Empirical optimization}
\item \textcolor{myred}{1000+ sampling steps required}
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{block}{Key Design Choices}
\begin{itemize}
\item Noise schedules ($\beta_t$, $\bar{\alpha}_t$)
\item Number of reverse steps
\item Conditioning strength
\item All determined empirically!
\end{itemize}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Research Gap: Lack of Theoretical Foundation}
\begin{block}{Current State - All Heuristic}
\begin{itemize}
\item \textbf{Noise schedules:} Linear, cosine - \textcolor{myred}{no theory}
\item \textbf{Sampling steps:} 50-1000 - \textcolor{myred}{empirical}
\item \textbf{Conditioning:} Fixed guidance weights - \textcolor{myred}{trial-and-error}
\item \textbf{Information flow:} Unknown - \textcolor{myred}{black box}
\end{itemize}
\end{block}

\vspace{0.5cm}
\begin{alertblock}{Key Research Questions}
\begin{enumerate}
\item How does information flow through diffusion process?
\item What are theoretical bounds on sampling complexity?
\item Can we design optimal schedules based on information theory?
\item How to optimize conditional generation adaptively?
\end{enumerate}
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Our Approach: Information-Theoretic Framework}
\begin{block}{Core Insight}
Diffusion models are \textbf{information channels} - we can measure how data information is lost and recovered during forward and reverse processes
\end{block}

\vspace{0.5cm}
\begin{block}{Information-Theoretic Analysis Pipeline}
\begin{enumerate}
\item \textcolor{myblue}{\textbf{Forward Process}} - Measure information loss via mutual information
\item \textcolor{mygreen}{\textbf{Optimal Scheduling}} - Design schedules for uniform information loss
\item \textcolor{myred}{\textbf{Sampling Bounds}} - Derive theoretical limits on step count
\item \textcolor{mypurple}{\textbf{Adaptive Conditioning}} - Optimize guidance strength
\end{enumerate}
\end{block}

\vspace{0.5cm}
\begin{alertblock}{Expected Outcomes}
\begin{itemize}
\item \textcolor{mygreen}{Principled design} replacing heuristics
\item \textcolor{mygreen}{Fewer sampling steps} (12-15 vs 1000+)
\item \textcolor{mygreen}{Better quality} through optimal information flow
\end{itemize}
\end{alertblock}
\end{frame}

\section{Mathematical Framework}

\begin{frame}
\frametitle{Forward Diffusion as Gaussian Channel}
\begin{block}{Mathematical Setup}
Forward (noising) process:
\begin{equation}
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim \mathcal{N}(0,\mathbf{I})
\end{equation}

where $\bar{\alpha}_t = \prod_{s=1}^{t}(1-\beta_s)$ is the \textbf{cumulative signal-to-noise factor}
\end{block}

\begin{block}{Information Flow}
\begin{itemize}
\item $t=0$: $\bar{\alpha}_0 \approx 1$ $\rightarrow$ clean data
\item $t=T$: $\bar{\alpha}_T \approx 0$ $\rightarrow$ pure noise
\item Information about $\mathbf{x}_0$ gradually lost
\end{itemize}
\end{block}

\begin{block}{Key Challenge}
How much information about $\mathbf{x}_0$ survives in $\mathbf{x}_t$?
\begin{equation}
I(\mathbf{x}_0; \mathbf{x}_t) = H(\mathbf{x}_t) - H(\mathbf{x}_t|\mathbf{x}_0)
\end{equation}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Mutual Information Formulation}
\begin{block}{Components of Mutual Information}
\begin{align}
I(\mathbf{x}_0; \mathbf{x}_t) &= H(\mathbf{x}_t) - H(\mathbf{x}_t|\mathbf{x}_0)
\end{align}
\end{block}

\begin{block}{Analytical vs. Intractable}
\begin{itemize}
\item \textbf{$H(\mathbf{x}_t|\mathbf{x}_0)$:} Analytically known (Gaussian)
\begin{equation}
H(\mathbf{x}_t|\mathbf{x}_0) = \frac{d}{2}\log(2\pi e (1-\bar{\alpha}_t))
\end{equation}

\item \textbf{$H(\mathbf{x}_t)$:} Intractable (mixture of Gaussians over unknown data distribution)
\end{itemize}
\end{block}

\begin{alertblock}{Solution: I-MMSE Identity}
Use Guo-Shamai-Verd\'u identity to compute MI without $H(\mathbf{x}_t)$:
\begin{equation}
\frac{d}{d\gamma} I(\mathbf{x}_0;\mathbf{x}_t) = \frac{1}{2}\text{MMSE}(\gamma), \quad \gamma_t = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t}
\end{equation}
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{I-MMSE Estimation Method}
\begin{block}{Practical Implementation}
\begin{enumerate}
\item Use trained DDPM denoiser $\boldsymbol{\varepsilon}_\theta(\mathbf{x}_t, t)$
\item Estimate posterior mean:
\begin{equation}
\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\varepsilon}_\theta(\mathbf{x}_t,t)\right)
\end{equation}
\item Compute empirical MMSE:
\begin{equation}
\widehat{\text{MMSE}}_t = \mathbb{E}[\|\mathbf{x}_0 - \hat{\mathbf{x}}_0(\mathbf{x}_t,t)\|^2]
\end{equation}
\item Integrate numerically to get full MI curve
\end{enumerate}
\end{block}

\begin{block}{Advantages}
\begin{itemize}
\item \textcolor{mygreen}{Tractable computation} without explicit entropy
\item \textcolor{mygreen}{Data-driven measure} of information flow
\item \textcolor{mygreen}{Model-based estimation} using trained denoiser
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Insights from MI Curve Analysis}
\begin{block}{Key Observations}
\begin{itemize}
\item \textbf{Early timesteps:} Steep MI drop $\rightarrow$ large information loss per step
\item \textbf{Later timesteps:} Slow decay $\rightarrow$ mostly redundant steps
\item \textbf{Common schedules:} Linear, cosine lose information unevenly
\end{itemize}
\end{block}

\begin{block}{Inefficiency of Traditional Schedules}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Schedule Type} & \textbf{Information Loss} & \textbf{Efficiency} \\
\hline
Linear & Uneven & Poor \\
Cosine & Uneven & Poor \\
\textbf{Our Approach} & \textbf{Uniform} & \textbf{Optimal} \\
\hline
\end{tabular}
\end{center}
\end{block}

\begin{alertblock}{Opportunity}
Design information-balanced schedules for uniform information loss per step
\end{alertblock}
\end{frame}

\section{Optimal Scheduling and Bounds}

\begin{frame}
\frametitle{Uniform Information Loss Principle}
\begin{block}{Design Goal}
Ensure each forward step removes the same amount of information:
\begin{equation}
\Delta I_t = I(\mathbf{x}_0;\mathbf{x}_{t-1}) - I(\mathbf{x}_0;\mathbf{x}_t) = \text{constant for all } t
\end{equation}
\end{block}

\begin{block}{Benefits}
\begin{itemize}
\item \textcolor{mygreen}{Predictable reconstruction difficulty}
\item \textcolor{mygreen}{More efficient sampling}
\item \textcolor{mygreen}{Stable training dynamics}
\item \textcolor{mygreen}{Fewer reverse steps needed}
\end{itemize}
\end{block}

\begin{block}{Implementation}
Adjust schedule parameters ($\beta_t$ or $\bar{\alpha}_t$) so that information loss per step is uniform across all timesteps
\end{block}
\end{frame}

\begin{frame}
\frametitle{Sampling Efficiency Bounds}
\begin{block}{Information Budget Perspective}
Treat $I(\mathbf{x}_0;\mathbf{x}_t)$ as an \textbf{information budget} the model must recover during generation
\end{block}

\begin{block}{Theoretical Lower Bound}
\begin{equation}
T_{\min} \geq \frac{I_{\text{required}}(D)}{I_{\text{per-step}}}
\end{equation}
where $I_{\text{required}}(D)$ is information needed for target distortion/quality $D$
\end{block}

\begin{block}{Practical Interpretation}
\begin{itemize}
\item Each reverse step recovers $\Delta I_t$ bits
\item Total steps must satisfy: $\sum_t \Delta I_t \approx I_{\text{data}}$
\item Determine minimum steps for desired FID quality level
\end{itemize}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Sampling Efficiency Results}
\begin{alertblock}{Key Result}
Information-balanced scheduling achieves state-of-the-art quality with only \textbf{12-15 sampling steps} vs. 50-1000 in traditional DDPMs
\end{alertblock}

\begin{block}{Impact}
\begin{itemize}
\item \textcolor{mygreen}{33-40\% reduction} in sampling steps
\item \textcolor{mygreen}{Maintains or improves} generation quality
\item \textcolor{mygreen}{Theoretical foundation} for step count selection
\item \textcolor{mygreen}{Practical efficiency gains} for real-world deployment
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{NFE Calculation: How We Measure Efficiency}
\begin{block}{Definition}
\textbf{NFE (Number of Function Evaluations)} = Number of times the denoising network $\boldsymbol{\varepsilon}_\theta(\mathbf{x}_t, t)$ is called during sampling
\end{block}

\begin{block}{Traditional DDPM Process}
\begin{enumerate}
\item Start with noise: $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$
\item For $t = T, T-1, \ldots, 1$:
   \begin{itemize}
   \item Call denoiser: $\hat{\boldsymbol{\epsilon}} = \boldsymbol{\varepsilon}_\theta(\mathbf{x}_t, t)$ \textcolor{myred}{(1 NFE)}
   \item Compute: $\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\boldsymbol{\epsilon}}\right) + \sigma_t \mathbf{z}$
   \end{itemize}
\item Total NFE = $T$ (number of timesteps)
\end{enumerate}
\end{block}

\begin{exampleblock}{Example: CIFAR-10 Generation}
\begin{itemize}
\item \textbf{Traditional DDPM:} $T = 1000$ timesteps $\rightarrow$ NFE = 1000
\item \textbf{DDIM:} $T = 50$ timesteps $\rightarrow$ NFE = 50  
\item \textbf{Our Method:} $T = 12$ timesteps $\rightarrow$ NFE = 12
\item \textbf{Efficiency Gain:} 1000/12 = 83Ã— fewer function calls!
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}
\frametitle{Adaptive Discretization Strategy}
\begin{block}{Key Insight from MI Curve}
Information changes fastest at certain timesteps $\rightarrow$ use adaptive step sizes
\end{block}

\begin{block}{Adaptive Strategy}
\begin{itemize}
\item \textbf{More steps} (smaller $\Delta t$) where information loss per step is high
\item \textbf{Fewer steps} where information changes slowly
\item \textbf{Minimal NFEs} while preserving image quality
\end{itemize}
\end{block}

\begin{block}{Implementation}
\begin{enumerate}
\item Compute MI curve $I(\mathbf{x}_0;\mathbf{x}_t)$
\item Identify regions of high/low information change
\item Allocate sampling steps adaptively
\item Optimize for target quality with minimal computation
\end{enumerate}
\end{block}
\end{frame}

\section{Conditional Generation Optimization}

\begin{frame}
\frametitle{Adaptive Guidance Strength}
\begin{block}{Problem with Fixed Guidance}
Standard classifier-free guidance uses fixed scale $s$:
\begin{equation}
\hat{\boldsymbol{\varepsilon}}_{\text{guided}} = (1+s)\hat{\boldsymbol{\varepsilon}}_{\text{uncond}} - s\hat{\boldsymbol{\varepsilon}}_{\text{cond}}
\end{equation}
But this constant $s$ is suboptimal - should vary across timesteps!
\end{block}

\begin{block}{Information-Theoretic Approach}
Define conditional mutual information:
\begin{equation}
r_t = \frac{I(\mathbf{x}_0; \mathbf{x}_t | y)}{I(\mathbf{x}_0; \mathbf{x}_t)}
\end{equation}
measures how much extra information the condition contributes
\end{block}

\begin{block}{Adaptive Guidance}
\begin{equation}
s_t = \lambda \cdot r_t = \lambda \cdot \frac{I(\mathbf{x}_0; \mathbf{x}_t | y)}{I(\mathbf{x}_0; \mathbf{x}_t)}
\end{equation}
where $\lambda$ is a global scaling factor
\end{block}
\end{frame}

\begin{frame}
\frametitle{Adaptive Guidance Behavior}
\begin{block}{Timestep-Dependent Behavior}
\begin{itemize}
\item \textbf{Early timesteps (high noise):} $r_t$ small $\rightarrow$ $s_t$ large $\rightarrow$ \textcolor{mygreen}{stronger guidance} to impose structure
\item \textbf{Later timesteps (low noise):} $r_t \approx 1$ $\rightarrow$ $s_t$ smaller $\rightarrow$ \textcolor{mygreen}{weaker guidance} to preserve diversity
\end{itemize}
\end{block}

\begin{block}{Benefits of Adaptive Guidance}
\begin{itemize}
\item \textcolor{mygreen}{Improved image fidelity} early in sampling
\item \textcolor{mygreen}{Prevents over-conditioning} and loss of diversity later
\item \textcolor{mygreen}{Reduces FID} and increases IS compared to fixed-$s$ baselines
\item \textcolor{mygreen}{Better fidelity-diversity trade-off}
\end{itemize}
\end{block}

\begin{block}{Optimal Weight Formula}
\begin{equation}
w^* = \sqrt{\frac{\mathcal{E}_T(\mathbf{c})}{1 - \mathcal{E}_T(\mathbf{c})}} \cdot \frac{\sigma_{\text{conditional}}}{\sigma_{\text{unconditional}}}
\end{equation}
where $\mathcal{E}_t(\mathbf{c}) = \frac{I(\mathbf{c}; \mathbf{x}_t)}{I(\mathbf{c}; \mathbf{x}_0)}$
\end{block}
\end{frame}

\section{Experimental Results}

\begin{frame}
\frametitle{Datasets and Experimental Setup}
\begin{block}{Datasets}
\begin{itemize}
\item \textbf{CIFAR-10:} 32$\times$32 RGB, 10 classes, 50K training
\item \textbf{CelebA-HQ:} 256$\times$256 faces, 30K images
\item \textbf{ImageNet:} 64$\times$64 downsampled, 1K classes, 1.28M training
\item \textbf{MS-COCO:} 256$\times$256 text-to-image, 118K images with captions
\end{itemize}
\end{block}

\begin{block}{Implementation Details}
\begin{itemize}
\item U-Net architecture with attention mechanisms
\item Information regularization weight $\lambda_{\text{info}} = 0.005$
\item AdamW optimizer, learning rate $2 \times 10^{-4}$
\item 8$\times$NVIDIA A100 (80GB) GPUs with mixed precision
\end{itemize}
\end{block}

\begin{block}{Evaluation Metrics}
\begin{itemize}
\item \textbf{FID:} Fr\'echet Inception Distance (lower is better)
\item \textbf{IS:} Inception Score (higher is better)
\item \textbf{NFE:} Number of Function Evaluations (lower is better)
\item \textbf{CLIP:} Text-image alignment for conditional generation
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Main Results: State-of-the-Art Performance}
\begin{table}
\centering
\caption{Comparison with state-of-the-art methods across all datasets}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc|ccc|ccc|cc}
\hline
\textbf{Method} & \multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c|}{\textbf{CelebA-HQ}} & \multicolumn{3}{c|}{\textbf{ImageNet}} & \multicolumn{2}{c}{\textbf{MS-COCO}} \\
& FID$\downarrow$ & IS$\uparrow$ & NFE & FID$\downarrow$ & LPIPS$\downarrow$ & NFE & FID$\downarrow$ & IS$\uparrow$ & NFE & FID$\downarrow$ & CLIP$\uparrow$ \\
\hline
DDPM & 3.17 & 9.46 & 1000 & 5.11 & 0.087 & 1000 & 7.72 & 9.51 & 1000 & 16.32 & 0.242 \\
Improved DDPM & 2.94 & 9.58 & 1000 & 4.73 & 0.083 & 1000 & 7.72 & 9.51 & 1000 & 15.77 & 0.248 \\
DDIM & 3.23 & 9.41 & 50 & 5.02 & 0.089 & 50 & 8.15 & 9.24 & 50 & 16.89 & 0.239 \\
Score SDE & 2.20 & 9.89 & 2000 & 2.92 & 0.074 & 2000 & 6.43 & 10.14 & 2000 & 14.23 & 0.267 \\
EDM & 1.97 & 9.84 & 18 & 2.44 & 0.076 & 18 & 2.44 & 10.01 & 18 & 12.63 & 0.251 \\
\hline
\textbf{Ours} & \textbf{1.82} & \textbf{10.14} & \textbf{12} & \textbf{2.31} & \textbf{0.069} & \textbf{15} & \textbf{2.18} & \textbf{10.37} & \textbf{14} & \textbf{11.85} & \textbf{0.283} \\
\hline
\end{tabular}%
}
\end{table}

\begin{block}{Key Achievements}
\begin{itemize}
\item \textcolor{mygreen}{Best FID scores} across all datasets
\item \textcolor{mygreen}{Fewest sampling steps} (12-15 vs. 18-2000)
\item \textcolor{mygreen}{Superior efficiency} (Quality/NFE ratio: 4.75 vs. 4.10)
\item \textcolor{mygreen}{State-of-the-art text-image alignment} (CLIP: 0.283)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Efficiency Analysis}
\begin{table}
\centering
\caption{Computational efficiency comparison (ImageNet 64$\times$64)}
\begin{tabular}{l|ccc|cc}
\hline
\textbf{Method} & NFE & Time (s) & FID$\downarrow$ & Quality/NFE$\uparrow$ & Quality/Time$\uparrow$ \\
\hline
DDPM & 1000 & 12.8 & 7.72 & 1.23 & 0.74 \\
DDIM-50 & 50 & 0.85 & 8.15 & 1.13 & 1.09 \\
DDIM-10 & 10 & 0.18 & 12.34 & 0.70 & 0.70 \\
EDM & 18 & 0.31 & 2.44 & 4.10 & 4.09 \\
Progressive Distillation & 8 & 0.14 & 8.50 & 1.04 & 1.03 \\
Consistency Models & 1 & 0.02 & 6.20 & 1.43 & 4.43 \\
\hline
\textbf{Ours} & \textbf{14} & \textbf{0.24} & \textbf{2.18} & \textbf{4.75} & \textbf{4.77} \\
\hline
\end{tabular}
\end{table}

\begin{block}{Efficiency Gains}
\begin{itemize}
\item \textcolor{mygreen}{Best quality-efficiency trade-off} (Quality/NFE: 4.75)
\item \textcolor{mygreen}{Fastest inference} (0.24s vs. 0.31s for EDM)
\item \textcolor{mygreen}{Theoretical bounds} enable near-optimal step selection
\item \textcolor{mygreen}{33-40\% reduction} in sampling steps while improving quality
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Ablation Studies}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{table}
\centering
\caption{Information regularization weight (CIFAR-10)}
\begin{tabular}{c|ccc|c}
\hline
$\lambda_{\text{info}}$ & FID$\downarrow$ & IS$\uparrow$ & NFE & Info Consistency \\
\hline
0.000 & 2.34 & 9.67 & 18 & 0.72 \\
0.001 & 2.08 & 9.89 & 15 & 0.84 \\
\textbf{0.005} & \textbf{1.82} & \textbf{10.14} & \textbf{12} & \textbf{0.91} \\
0.010 & 1.95 & 10.02 & 12 & 0.89 \\
0.050 & 2.41 & 9.78 & 13 & 0.87 \\
\hline
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.5\textwidth}
\begin{table}
\centering
\caption{Noise schedule comparison}
\begin{tabular}{l|cc}
\hline
\textbf{Schedule} & FID$\downarrow$ & NFE \\
\hline
Linear & 2.15 & 20 \\
Cosine & 1.98 & 18 \\
\textbf{Uniform Info Loss} & \textbf{1.82} & \textbf{12} \\
Rate-Distortion & 1.87 & 13 \\
\hline
\end{tabular}
\end{table}
\end{column}
\end{columns}

\begin{block}{Key Findings}
\begin{itemize}
\item Optimal $\lambda_{\text{info}} = 0.005$ balances quality and consistency
\item Uniform information loss schedule consistently outperforms alternatives
\item Information-theoretic guidance significantly improves performance
\item MINE estimation provides most accurate guidance (91\% accuracy)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Conditional Generation Results}
\begin{table}
\centering
\caption{Text-to-image generation on MS-COCO}
\begin{tabular}{l|ccc|c}
\hline
\textbf{Guidance Method} & FID$\downarrow$ & CLIP$\uparrow$ & IS$\uparrow$ & Diversity$\uparrow$ \\
\hline
No Guidance & 18.45 & 0.198 & 8.23 & 0.84 \\
CFG (w=7.5) & 12.63 & 0.251 & 9.67 & 0.62 \\
CFG (w=15.0) & 11.82 & 0.264 & 10.14 & 0.45 \\
\textbf{Ours (Optimal w*)} & \textbf{11.85} & \textbf{0.283} & \textbf{10.52} & \textbf{0.71} \\
\hline
\end{tabular}
\end{table}

\begin{block}{Advantages of Optimal Guidance}
\begin{itemize}
\item \textcolor{mygreen}{Superior text-image alignment} (CLIP: 0.283 vs. 0.251)
\item \textcolor{mygreen}{Better diversity preservation} (0.71 vs. 0.45)
\item \textcolor{mygreen}{Adaptive strength} based on conditioning effectiveness
\item \textcolor{mygreen}{Prevents over-conditioning} and mode collapse
\end{itemize}
\end{block}

\begin{block}{Information Flow Validation}
Theoretical predictions match empirical measurements with correlation coefficient $r = 0.94$ across all timesteps
\end{block}
\end{frame}

\begin{frame}
\frametitle{Robustness Analysis}
\begin{table}
\centering
\caption{Robustness to dataset scale (CIFAR-10 subset experiments)}
\begin{tabular}{c|ccc|ccc}
\hline
\textbf{Dataset Size} & \multicolumn{3}{c|}{\textbf{Baseline (DDIM-50)}} & \multicolumn{3}{c}{\textbf{Ours (Info-Guided)}} \\
& FID$\downarrow$ & IS$\uparrow$ & NFE & FID$\downarrow$ & IS$\uparrow$ & NFE \\
\hline
5K (10\%) & 8.45 & 8.12 & 50 & 5.23 & 9.01 & 18 \\
10K (20\%) & 6.78 & 8.67 & 50 & 3.95 & 9.45 & 16 \\
25K (50\%) & 4.89 & 9.15 & 50 & 2.87 & 9.89 & 14 \\
50K (100\%) & 3.23 & 9.41 & 50 & 1.82 & 10.14 & 12 \\
\hline
\end{tabular}
\end{table}

\begin{block}{Robustness Findings}
\begin{itemize}
\item \textcolor{mygreen}{Consistent improvements} across all dataset scales
\item \textcolor{mygreen}{Advantage increases} with larger datasets
\item \textcolor{mygreen}{Better out-of-distribution performance} (91\% vs. 89\% relative performance)
\item \textcolor{mygreen}{Graceful degradation} outside optimal parameter ranges
\end{itemize}
\end{block}
\end{frame}

\section{Theoretical Contributions}

\begin{frame}
\frametitle{Theoretical Guarantees and Assumptions}
\begin{block}{Key Assumptions}
\begin{enumerate}
\item \textbf{Gaussian Data:} Data can be approximated by high-dimensional Gaussian distributions
\item \textbf{Perfect Denoising:} Theoretical bounds assume perfect denoising networks
\item \textbf{Independence:} Noise at different timesteps is independent
\end{enumerate}
\end{block}

\begin{block}{Theoretical Guarantees}
\begin{enumerate}
\item \textbf{Theorem 1 (Optimal Schedule Convergence):} Uniform information loss schedule minimizes expected sampling steps for given reconstruction quality
\item \textbf{Theorem 2 (Information Bound Tightness):} Sampling complexity bound is tight within constant factor for Gaussian data
\item \textbf{Theorem 3 (Conditioning Optimality):} Optimal guidance strength maximizes mutual information between conditions and generated samples
\end{enumerate}
\end{block}

\begin{alertblock}{Practical Impact}
These guarantees provide theoretical foundation for principled design choices rather than empirical optimization
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{Computational Methods}
\begin{block}{Neural Mutual Information Estimation}
Extended MINE approach for diffusion models:
\begin{align}
\hat{I}_{MINE}(\mathbf{x}_0; \mathbf{x}_t) &= \max_\phi \; \mathbb{E}_{(\mathbf{x}_0, \mathbf{x}_t) \sim p}[T_\phi(\mathbf{x}_0, \mathbf{x}_t)] \\
&\quad - \log \; \mathbb{E}_{(\mathbf{x}_0, \mathbf{x}_t) \sim p \otimes p}[e^{T_\phi(\mathbf{x}_0, \mathbf{x}_t)}]
\end{align}
\end{block}

\begin{block}{Specialized Architecture}
\begin{equation}
T_\phi(\mathbf{x}_0, \mathbf{x}_t) = \text{MLP}(\text{concat}(\text{Encoder}(\mathbf{x}_0), \text{Encoder}(\mathbf{x}_t)))
\end{equation}
\end{block}

\begin{block}{Alternative Methods}
\begin{itemize}
\item \textbf{Contrastive Estimation:} Faster but less accurate (85\% vs. 91\%)
\item \textbf{Variational Approximation:} Real-time analysis during training (79\% accuracy)
\item \textbf{MINE:} Most accurate but higher computational cost
\end{itemize}
\end{block}
\end{frame}

\section{Conclusion and Impact}

\begin{frame}
\frametitle{Key Contributions Summary}
\begin{block}{Theoretical Contributions}
\begin{enumerate}
\item \textbf{First information-theoretic framework} for diffusion model analysis
\item \textbf{Optimal noise scheduling} based on uniform information loss principle
\item \textbf{Theoretical sampling bounds} for quality-efficiency trade-offs
\item \textbf{Optimal conditioning} with adaptive guidance strength
\item \textbf{Computational methods} for high-dimensional information estimation
\end{enumerate}
\end{block}

\begin{block}{Practical Contributions}
\begin{enumerate}
\item \textbf{State-of-the-art performance} across all evaluated datasets
\item \textbf{33-40\% reduction} in sampling steps while improving quality
\item \textbf{Comprehensive validation} on multiple datasets and tasks
\item \textbf{Principled design} replacing empirical optimization
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Broader Impact and Significance}
\begin{block}{Paradigm Shift}
\begin{itemize}
\item \textcolor{myblue}{From empirical to principled design}
\item \textcolor{myblue}{Unified theoretical foundation} for diffusion models
\item \textcolor{myblue}{Predictable scaling behavior}
\item \textcolor{myblue}{Connects diffusion models to classical information theory}
\end{itemize}
\end{block}

\begin{block}{Practical Implications}
\begin{itemize}
\item \textcolor{mygreen}{More accessible diffusion models for deployment}
\item \textcolor{mygreen}{Principled architecture design decisions}
\item \textcolor{mygreen}{Guidance for new domains and applications}
\item \textcolor{mygreen}{Democratizes high-quality generative modeling}
\end{itemize}
\end{block}

\begin{block}{Research Impact}
\begin{itemize}
\item \textcolor{myred}{Opens new research directions in generative AI}
\item \textcolor{myred}{Enables transfer of information theory insights}
\item \textcolor{myred}{Provides tools for systematic model analysis}
\item \textcolor{myred}{Establishes theoretical foundations for future work}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Limitations and Future Work}
\begin{block}{Current Limitations}
\begin{itemize}
\item \textcolor{myred}{Gaussian assumption} for analytical tractability
\item \textcolor{myred}{High-dimensional estimation challenges}
\item \textcolor{myred}{Computational overhead} (20-40\% training time increase)
\item \textcolor{myred}{Limited theoretical guarantees} for complete algorithm
\end{itemize}
\end{block}

\begin{block}{Future Research Directions}
\begin{enumerate}
\item \textbf{Beyond Gaussian:} f-divergences, optimal transport
\item \textbf{Efficient estimation:} Hierarchical schemes, neural ODEs
\item \textbf{Multi-modal extensions:} Joint generation across modalities
\item \textbf{Formal convergence:} Theoretical guarantees
\item \textbf{Alternative paradigms:} Flows, autoregressive, consistency models
\item \textbf{Adaptive information processing:} Dynamic adjustment based on content complexity
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Final Results Summary}
\begin{center}
\Large
\textbf{Information Theory Provides the Key to Understanding and Optimizing Diffusion Model Dynamics}
\end{center}

\vspace{1cm}

\begin{block}{Achievement Summary}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{FID (Ours)} & \textbf{NFE (Ours)} & \textbf{Best Baseline} \\
\hline
CIFAR-10 & \textbf{1.82} & \textbf{12} & EDM: 1.97 (18 NFE) \\
CelebA-HQ & \textbf{2.31} & \textbf{15} & EDM: 2.44 (18 NFE) \\
ImageNet & \textbf{2.18} & \textbf{14} & EDM: 2.44 (18 NFE) \\
MS-COCO & \textbf{11.85} & \textbf{15} & EDM: 12.63 (18 NFE) \\
\hline
\end{tabular}
\end{center}
\end{block}

\vspace{0.5cm}

\begin{block}{Key Takeaways}
\begin{itemize}
\item \textcolor{mygreen}{First comprehensive framework} for information-theoretic analysis
\item \textcolor{mygreen}{Theoretical foundations} enable principled design choices
\item \textcolor{mygreen}{State-of-the-art results} with improved efficiency
\item \textcolor{mygreen}{New research directions} in generative AI
\end{itemize}
\end{block}

\vspace{0.5cm}

\begin{center}
\Large
\textbf{Thank you for your attention!}
\end{center}
\end{frame}

\end{document}
