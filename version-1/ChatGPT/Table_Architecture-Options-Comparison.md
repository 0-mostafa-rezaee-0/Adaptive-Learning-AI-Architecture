| **Option**                                                                                                                                                                                                            | **Components & Approach**                                                                                                                                                                                                                                                                                                                                                                                                         | **Cost & Infra**                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Latency**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Data Needs & Cold-Start**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Expected Learning Impact**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Risks/Cons**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | **Team Fit**                                                                                                                                                                                                                                                                                                                                                  |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **A) RAG-first + ReRank + Agent Orchestration**  <br>*Baseline Retrieval-Augmented Generation pipeline with an agent coordinating steps*                                                                              | - Bi-encoder vector search over content (embeddings index) <br>- BM25 + dense *hybrid* retrieval for coverage <br>- Cross-encoder reranker for top-k results <br>- Agent orchestrator (retriever → answer composer) to ensure grounded answers                                                                                                                                                                                    | **Low initial cost:** Uses pre-trained LLM and existing content. Build vector index, no custom model training initially. <br>Standard cloud VMs for embedding & LLM calls; no special GPUs needed unless heavy traffic.                                                                                                                                                                                                                                                        | **Moderate:** Retrieval (~100ms per query), rerank (~10–50ms for cross-encoder on top-10), plus LLM generation (~seconds depending on model). End-to-end typically a few seconds per question.                                                                                                                                                                                                                                                                                                            | **Cold-start ready:** Requires *no* historical training data for recommendations – uses semantic similarity and keywords to find relevant snippets. <br>Content corpus (videos, PDFs) and embedding model needed upfront. <br>**Data needs:** minimal for start; improves with feedback but not required to function.                                                                                                                                                                                                                                                                                                     | **Good baseline learning:** Delivers relevant, *grounded* answers to student queries, reducing misinformation. Learning impact initially comes from giving correct info quickly. <br>Less personalization in this option – all users get similar content for a given query.                                                                                                                                                                                                                                                                                                                             | - **Knowledge gaps:** Depends on existing content; if knowledge base lacks an answer, the LLM may still falter or omit (mitigated by saying “I don’t know”). <br>- **Not personalized:** Doesn’t adapt to student’s prior knowledge or difficulty needs. <br>- **Maintenance:** Content updates must be re-indexed, but this is manageable.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | **Strong:** Team has NLP and “agentic AI” skills. Setting up retrieval and prompt orchestration is straightforward. Leverages LLM expertise without requiring new model training.                                                                                                                                                                             |
| **B) Lightweight Fine-Tuning (LoRA/PEFT) + RAG**  <br>*Train small adapters for pedagogical style or domain, layered on RAG pipeline*                                                                                 | - RAG pipeline as above for factual grounding. <br>- Add LoRA adapters on the LLM for specific abilities: e.g. more explanatory tone, Socratic questioning style, or domain-specific vocabulary. <br>- Possibly fine-tuned modules for distractor generation or hinting.                                                                                                                                                          | **Moderate cost:** Requires training small adapter (~<1% of model parameters). One-time fine-tuning per task (question style, etc.) on either domain data or synthetic examples. <br>Compute: a single GPU can fine-tune a LoRA on a 7B–13B model in a few hours.                                                                                                                                                                                                              | **Same or Slightly higher:** Inference uses base LLM + adapter merge; negligible overhead (just adds a few matrix ops). No extra network calls. <br>Latency remains dominated by LLM runtime (seconds per answer).                                                                                                                                                                                                                                                                                        | **Cold-start:** Adapters need examples. If no real data, can use *few-shot prompt* or synthetic data to do a “proto-fine-tune.” Cold-start viability is okay if using general model behavior initially, then fine-tune when data available. <br>**Data needs:** Low/medium – on the order of hundreds of example Q&A or conversations in desired style. Can incorporate expert-written samples.                                                                                                                                                                                                                           | **Higher learning relevance:** Fine-tuning can enforce pedagogical best-practices (e.g. always include a hint or ensure explanations are at the right level). This yields more *consistent, student-friendly outputs* than prompt-tuning alone. <br>Likely boosts student engagement and comprehension (e.g. clearer explanations, better-aligned questions).                                                                                                                                                                                                                                           | - **Data dependency:** Requires quality training data for the specific style/skill (which we may need to create). <br>- **Risk of minor drift:** Poor fine-tuning could degrade base model correctness (mitigated by LoRA freezing most weights). <br>- **Maintenance:** If base model is updated, adapters might need retraining or adjustment for compatibility.                                                                                                                                                                                                                                                                                                                                                                                                                                                            | **Good:** Team can handle fine-tuning on domain data. PEFT techniques are well within data scientists’ expertise. Leverages our strength in model fine-tuning while keeping costs manageable.                                                                                                                                                                 |
| **C) RL/Bandits for Content Selection on RAG baseline**  <br>*Contextual bandit policy learns which content piece (or strategy) yields best learning outcome, on top of retrieval results*                            | - Use RAG retrieval as default candidate generator. <br>- Train a bandit or RL policy to choose the *best* content snippet or next action for a student given context (user history, question difficulty, etc.). <br>- Reward defined by student’s improvement (quiz correctness, time-on-task) instead of clicks. <br>- Could start with epsilon-greedy or LinUCB bandit, later a policy-gradient RL as data grows.              | **Upfront cost:** Higher complexity. Need to log interactions, maintain a policy model. Infrastructure for online learning (continuous training) or at least periodic policy updates. <br>Compute: Bandit algorithms are lightweight (linear models), but full RL (if used) might require GPU for training simulations.                                                                                                                                                        | **Low runtime overhead:** Policy lookup is fast (milliseconds to select an action). The main latency remains the content retrieval and LLM. <br>**Potential delay:** Slightly longer decision-making if exploring (e.g. computing scores for multiple options). Negligible for user.                                                                                                                                                                                                                      | **Cold-start:** *Challenging.* Needs some initial policy – likely we’d start with heuristic (e.g. always pick highest-ranked minimal resource) and a small exploration rate. As data accrues, the bandit learns. <br>**Data needs:** Continuous interaction data (which we will get as students use the system). Off-policy learning possible from historical Q&A logs (if any reward proxies exist there). Initially, can simulate or use expert judgments as reward signals.                                                                                                                                            | **High adaptive impact:** Over time, the system will *learn which content leads to the greatest learning gains*. E.g., it might learn that for concept X, a 3-minute video yields higher quiz scores than a text snippet, adjusting recommendations accordingly. Prior studies show bandit policies can boost educational outcomes by personalizing content sequencing. <br>Bandits also enable exploration of new content, finding better resources than a static rank might choose.                                                                                                                   | - **Complexity:** Requires careful reward design (to truly reflect learning, not just short-term answers). Mis-specified rewards could cause the policy to exploit the wrong behavior (e.g. always show easiest content to get quick correct answers – we will mitigate by rewarding mastery gains, not raw score alone). <br>- **Data hunger:** Until sufficient student interactions are collected, policy may be suboptimal or unstable. Cold-start period must be managed so students aren’t harmed by random exploration (use safe initial policy + high exploration decay). <br>- **Evaluation:** Off-policy evaluation is non-trivial; we’ll use techniques like IPS to validate the policy offline. Safety and fairness of learned policy must be monitored (ensuring it doesn’t, for example, favor certain groups). | **Strong:** The team’s reinforcement learning experts can design and tune the bandit. They have experience with contextual bandit algorithms and can manage the engineering of an online learning loop. This fits our ambition to push an *adaptive* AI tutor.                                                                                                |
| **D) Fully Fine-Tuned Task-Specific Models (small LMs)**  <br>*Replace large LLM + prompts with an ensemble of smaller models each fine-tuned for a specific subtask (retrieval, question generation, grading, etc.)* | - e.g. Use a  Encoder-decoder model (like T5 or smaller GPT) fine-tuned for question generation on our content. <br>- A BERT or RoBERTa fine-tuned for short answer grading against rubrics. <br>- Perhaps a smaller transformer for difficulty estimation or hint generation. <br>- Essentially, a pipeline where each stage is a custom model trained on labeled data for that task (instead of prompting a big general model). | **High upfront cost:** Each model needs training data and pipeline integration. Fine-tuning small models is cheaper than a giant model, but doing many of them (QG, grading, etc.) adds up. Infrastructure becomes a collection of services (one per model) – more complex to maintain. <br>In deployment, smaller models can run on CPU or single GPU, which might reduce per-request cost if optimized. But the cost lies in development and maintenance of multiple models. | **Potentially faster inference:** Each small model is optimized for its task and could be very fast (milliseconds) compared to a big LLM taking seconds. Pipeline parallelism could further speed it up (e.g. generating question and evaluating previous answer concurrently). <br>However, pipeline overhead (data passing between models) and serial stages might negate some gains. Worst-case latency is sum of all stages. Still, likely slightly better than using one large model for everything. | **Cold-start:** *Poor.* This approach requires substantial labeled data to train each component before it’s useful. We lack these at project start (no “correct question → best video segment” training pairs, etc.). Would need a major data labeling effort or rely on synthetic data to train models. <br>**Data needs:** High – for each subtask, hundreds or thousands of examples. E.g., to train a question generator to cover Bloom’s levels, we’d need a dataset of content passages with questions at various cognitive levels. For grading, need student answers with scores. These are non-trivial to obtain. | **Moderate impact (if achieved):** Specialized models can outperform prompts in narrow tasks once fully trained. For example, a fine-tuned question generator might consistently produce well-structured, curriculum-aligned questions, and a grading model can reliably score answers like a human. This could improve the consistency of the system’s teaching and assessment. <br>However, the *ceiling* of each small model may be lower than a big LLM, especially for creative tasks. So while reliability goes up, the richness or adaptivity might be limited to what the model was trained on. | - **Long development cycle:** Building each component from scratch increases integration points (potential bugs) and slows iteration. Any change in one task (e.g. new question style) requires collecting new data and retraining that model. <br>- **Maintenance burden:** Multiple models to update as content or objectives change. Ensuring they all remain compatible (e.g. question generator’s style matches the grader’s expectations) is challenging. <br>- **Quality risk:** Smaller models may not capture the full nuance. E.g., a 110M parameter model fine-tuned for grading might not understand complex reasoning as well as GPT-4 with a prompt. Could lead to lower-quality feedback unless data is extremely good.                                                                                        | **Feasible but heavy:** The team has the skills to train models, but doing many at once spreads our effort thin. It diverts focus from leveraging powerful existing LLMs. Given our size, a fully bespoke model stack is risky. We can pilot one small model (e.g. a BERT grader) later on, but not make the entire system from only custom models initially. |
