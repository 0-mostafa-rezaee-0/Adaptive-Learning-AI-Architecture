```latex
\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{
    a4paper,
    margin=1in,
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{enumitem}

\title{\textbf{Adaptive Learning System Roadmap: Agentic AI and Evidence-Based Learning Gain}}
\author{Principal AI Architect}
\date{\today}

\begin{document}
\maketitle
\thispagestyle{empty}

\tableofcontents
\newpage

\section{Executive Summary}

The recommended approach is a staged hybrid architecture combining Retrieval-Augmented Generation (RAG) for factual grounding and minimality, and Contextual Bandits (CB) for optimizing content selection based on measured learning gains. This strategy addresses the cold-start challenge while ensuring the system learns to prioritize pedagogical effectiveness over mere relevance.

\begin{itemize}[noitemsep]
    \item \textbf{Staged Hybrid Architecture:} Start RAG-first (M1-M3) for rapid prototyping and grounding, then implement Contextual Bandits (M4) for optimization.
    \item \textbf{Minimality Constraint:} Enforce hard limits and prioritize resource segments (1--3 minute video clips, 1--2 page PDF snippets) over whole assets using ASR and semantic segmentation tools (M2).
    \item \textbf{Learning Gain Metric:} Primary metric is $\Delta\theta$, the change in latent learner ability over sequential sessions, measured using Item Response Theory (IRT).
    \item \textbf{Cold-Start Mitigation:} Use heuristics, metadata filters, and ASR/semantic weak labeling to kickstart content recommendations in the absence of historical behavioral data (M1/M2).
    \item \textbf{Agentic Orchestration:} Utilize a Plan-Act-Evaluate loop to dynamically select content, generate formative questions, assess learning, and determine the next optimal action.
    \item \textbf{Agility through PEFT:} Avoid full fine-tuning of large models. Use Parameter Efficient Fine-Tuning (PEFT/LoRA) for smaller, task-specific models (Question Generation, Grading Rubrics) to maintain agility during foundation model updates (M5).
    \item \textbf{Data Leverage:} Pre-train pedagogical components (question difficulty, rubric grading) using existing historical Q\&A and assessment data (M3).
    \item \textbf{Measurable Milestones:} Each 2-week milestone delivers an A/B testable increment with clearly defined offline and online metrics (e.g., nDCG, Overkill Rate, Item Discrimination).
    \item \textbf{Scalability:} RAG components are inherently scalable; Bandits provide robust online optimization without requiring massive, expensive RL environments.
\end{itemize}

\section{Architecture Options Comparison}

The following table compares at least three viable architecture options against the required constraints and capabilities.

\begin{longtable}{p{2.5cm}p{3.5cm}p{3.5cm}p{3.5cm}}
\caption{Alternative Architecture Options Comparison} \\
\toprule
\textbf{Feature} & \textbf{Option A: RAG + Agentic Orchestration (Baseline)} & \textbf{Option C: RL/Bandits on RAG Baseline (Recommended End-State)} & \textbf{Option B: LoRA FT for Pedagogy/Style + RAG} \\
\midrule
\endfirsthead
\caption{Alternative Architecture Options Comparison (Cont.)} \\
\toprule
\textbf{Feature} & \textbf{Option A: RAG + Agentic Orchestration (Baseline)} & \textbf{Option C: RL/Bandits on RAG Baseline (Recommended End-State)} & \textbf{Option B: LoRA FT for Pedagogy/Style + RAG} \\
\midrule
\endhead
\bottomrule
\endfoot
\endlastfoot

\textbf{Components} & Dense Retrieval, Cross-Encoder Reranker, Agentic Planner (LLM), Prompt Engineering for Pedagogy. & RAG stack + Contextual Bandit Policy (Exploration/Exploitation), Reward Service, Off-Policy Evaluator. & RAG stack + LoRA Adapters for Question Generation/Distractor generation (small LLMs/T5). \\
\textbf{Infra} & Standard vector DB, serving LLM (e.g., Gemini Pro), high-throughput inference for reranker. & Adds real-time policy server (e.g., Vowpal Wabbit, custom service), extensive telemetry/logging. & Standard RAG + hosting for smaller fine-tuned models/adapters, requiring specialized FT pipelines. \\
\textbf{Cost} & Moderate (LLM inference is the primary driver). & High Operational Cost (Telemetry, Policy Updates) but potentially high ROI from optimization. & High Setup Cost (Labeling/Training) but potentially lower LLM inference cost if using specialized small LMs. \\
\textbf{Latency} & Standard RAG latency (0.5s -- 2s). & Similar to RAG, plus Policy evaluation (negligible if optimized). & Lower latency for specialized tasks (Q-Gen) if utilizing optimized small models/adapters. \\
\textbf{Data Needs} & Low (relies on FMs). Needs labeled data for initial cross-encoder training. & High (Requires large volumes of logged user interactions, content choices, and outcome metrics). & High (Requires thousands of high-quality, labeled pedagogical examples). \\
\textbf{Cold-Start Viability} & Excellent. Depends only on semantic matching and heuristics. & Poor for optimization layer. Must be layered on top of a successful RAG (M4 start). & Moderate. Requires initial exemplar set for FT. \\
\textbf{Expected Learning Impact} & Good (Factual relevance). Cannot intrinsically optimize for $\Delta\theta$. & Excellent (Directly optimizes content selection for maximal learning gain $\Delta\theta$). & Good (Improves question consistency and quality, leading to better measurement/assessment). \\
\textbf{Risks} & Prompt drift, reliance on LLM consistency, inability to optimize minimality vs. learning trade-off effectively. & Requires sophisticated causal inference/OPE, risk of exploiting policy to maximize simple metrics (gaming). & High setup cost, risk of adapter incompatibility upon FM upgrade, complexity of managing multiple FT models.
\end{longtable}

\section{Final Recommended Architecture}

The recommended approach is the staged implementation of Option C, built on a robust RAG and Agentic foundation (Option A).

\subsection{Architectural Components}
\begin{enumerate}
    \item \textbf{Learner Interface/Question:} Student asks a question.
    \item \textbf{Agentic Orchestration (Planner):} Determines the current learner state ($\theta$ via IRT model) and plans the next action (Retrieve Resource $\rightarrow$ Generate Q $\rightarrow$ Assess).
    \item \textbf{Retrieval Engine (RAG):}
    \begin{itemize}
        \item \textbf{Chunking/Metadata:} Semantic chunking for PDFs (by section/paragraph) and ASR/Scene Detection for videos (producing 1-3 min segments). Metadata includes duration, Bloom level coverage, prior usage, and difficulty (initial heuristic based on topic).
        \item \textbf{Hybrid Search:} BM25 (keyword/lexical) + Dense Embedding Search (semantic).
        \item \textbf{Reranking:} Bi-encoder retrieval (fast initial set) $\rightarrow$ Cross-Encoder Reranker (BERT-based) for relevance $\rightarrow$ Maximal Marginal Relevance (MMR) for diversification/sufficiency.
    \end{itemize}
    \item \textbf{Content Minimization Engine:} Applies hard constraints (e.g., $\le$ 3 minutes or $\le$ 2 pages). Calculates a "Sufficiency Score" (semantic coverage relative to the question) / Duration to rank for minimality. Selects only the Top-K segments.
    \item \textbf{Content Selection Policy (Contextual Bandit/RL):} Based on the current learner state (context), selects the single best minimal resource segment to maximize the predicted reward (Learning Gain Proxy).
    \item \textbf{Pedagogical Layer (Generative Agents):} Uses a smaller LoRA-tuned LLM for consistency.
    \begin{itemize}
        \item \textbf{Question Generation:} Generates $N$ formative questions based on Bloom's Taxonomy, guaranteed to be grounded in the selected resource. Includes distractors and hints.
        \item \textbf{Rubric-Based Grading:} Assesses student answers against a reference and predefined rubric/ontology, providing structured feedback.
    \end{itemize}
    \item \textbf{Assessment \& Learning Analytics (IRT):} Updates learner latent ability ($\theta$) and question parameters ($a, b, c$) based on formative assessment results, feeding back into the Agentic Planner.
    \item \textbf{Telemetry \& Feedback Loop:} Logs every interaction, resource choice, and resulting assessment score, providing the data for the Bandit/RL policy updates.
\end{itemize}

\subsection{Architecture Diagram (Textual Representation)}

\begin{figure}[h!]
\centering
\begin{verbatim}
[Learner Question] -> [Agentic Planner]
                               | (θ state, plan)
                               V
+-------------------------------------------------------+
| 1. Retrieval Engine (Hybrid Search + Cross-Reranker)  |
+-------------------------------------------------------+
                               | (Top-N segments)
                               V
+-------------------------------------------------------+
| 2. Content Minimization (Sufficiency/Duration Score)  |
+-------------------------------------------------------+
                               | (Minimal Candidates)
                               V
+-------------------------------------------------------+
| 3. Content Selection Policy (Contextual Bandit)       |
|    Reward: Δθ Proxy + Minimality Bonus                |
+-------------------------------------------------------+
                               | (Selected minimal resource)
                               V
[Learner views resource] -> [4. Pedagogical Layer (Q-Gen LoRA)]
                               |
                               V
[Formative Assessment] -> [5. Assessment Layer (IRT Model)]
                               | (New θ, Item Parameters)
                               V
[Telemetry Log/Reward Service] -> [Bandit Policy Update]
\end{verbatim}
\caption{Recommended Adaptive Learning Architecture Flow}
\end{figure}

\section{Data Plan (Cold-Start to Flywheel)}

\subsection{Cold-Start for Content Recommendations}
Since historical recommendation data is absent, we must rely on heuristics and semantic analysis initially:
\begin{enumerate}
    \item \textbf{Heuristics \& Metadata Filters:} Filter content based on essential metadata (Course ID, Topic Tags, Bloom Level appropriateness).
    \item \textbf{ASR \& Semantic Weak Labels:} Use ASR transcription (for video) and section headers (for PDF) combined with semantic embedding coverage to create weak labels indicating which segments of content cover which concepts.
    \item \textbf{Initial Reranker Training:} Train the initial Cross-Encoder Reranker using synthetic data generated by pairing high-quality existing questions (from historical Q\&A logs) with semantically aligned content segments.
    \item \textbf{Teacher-in-the-Loop (M1/M2):} In the prototype phase, implement an initial low-volume feedback loop where subject matter experts (SMEs) score the minimality and relevance of the top-5 resource suggestions for a random sample of queries.
\end{enumerate}

\subsection{Rapid Dataset Creation and Labeling}
\begin{enumerate}
    \item \textbf{Offline Labeling Protocols:} Define strict rubrics for "Best Minimal Resource" (e.g., must cover X concepts, must not exceed Y duration, scored 1-5). Use SMEs for high-precision labeling.
    \item \textbf{Active Learning Loops:} Once the RAG is live, use user behavior (CTR, minimal resource consumption time, subsequent quiz performance) as proxy labels. Prioritize labeling for queries where the RAG/Bandit policy uncertainty is highest (active learning).
    \item \textbf{Inter-Rater Agreement (IRA):} Ensure high IRA ($\kappa > 0.7$) among SMEs used for labeling core Reranker and Pedagogical samples.
\end{enumerate}

\subsection{Leveraging Existing Q\&A/Assessment Logs}
\begin{enumerate}
    \item \textbf{Question Generation Refinement:} Use historical Q\&A pairs (high-quality questions + canonical answers) as exemplars for few-shot prompting and later LoRA fine-tuning (M3/M5). This ensures generated questions align with the established pedagogical style.
    \item \textbf{IRT Parameter Calibration:} Use historical assessment data (item responses) to pre-calibrate initial Item Response Theory (IRT) parameters ($a, b, c$). This provides a stable baseline for measuring learner ability ($\theta$) from M3 onwards.
    \item \textbf{Grading Rubrics:} Extract common errors and correct answer variations from historical student responses to refine the automatic rubric grader for better precision.
\end{enumerate}

\section{Metrics \& Evaluation}

Metrics must clearly distinguish between technical performance, efficiency, and true learning outcomes.

\subsection{Technical \& Efficiency Metrics (M1, M2)}
\begin{itemize}[noitemsep]
    \item \textbf{Retrieval/Selection:} nDCG@k (Normalized Discounted Cumulative Gain), Mean Average Precision (MAP), Recall@k.
    \item \textbf{Coverage:} Percentage of user questions for which the RAG system finds at least one highly relevant resource (Relevance Score $> 0.8$).
    \item \textbf{Latency:} Time-to-first-useful-resource (P95 latency of the full RAG pipeline).
    \item \textbf{Minimality:} Median resource length (Target: $\le 3$ min or $\le 2$ pages). "Overkill Rate" (\% of suggestions exceeding target length). Compression Ratio (Length of suggested segment / Length of full asset).
\end{itemize}

\subsection{Pedagogical \& Assessment Quality Metrics (M3)}
\begin{itemize}[noitemsep]
    \item \textbf{Question Quality (Offline):} Expert rubric scores (Clarity, Alignment to Resource, Bloom Level). Pass@k on canonical answers (automatic verification against ground truth). Factuality via Reference-Grounded checks.
    \item \textbf{Assessment Quality (IRT):}
    \begin{itemize}
        \item Item Discrimination ($a$): Measures how well an item distinguishes between high and low-ability students. Target $a>0.8$.
        \item Item Difficulty ($b$): Ensures generated questions cover an appropriate difficulty range.
        \item Ability ($\theta$) Metrics: Stability (low standard error of measurement, SE) and Predictive Validity (correlation between $\theta$ and downstream course grade).
    \end{itemize}
\end{itemize}

\subsection{Learning Outcome Metrics (M4, M5, M6)}
These are the primary success metrics for the entire system.
\begin{itemize}[noitemsep]
    \item \textbf{Latent Ability Change ($\Delta\theta$):} The primary metric. Measured as the mean or median difference in estimated learner ability ($\theta$) between the start of a session and the end, after consuming the resource and completing formative questions.
    \item \textbf{Mastery Progression:} Time/sessions required for learners to move from Novice ($\theta < -1$) to Proficient ($\theta > 0$) in specific topics.
    \item \textbf{Normalized Gain ($g$):} $g = (\text{Posttest Score} - \text{Pretest Score}) / (100 - \text{Pretest Score})$. Use sequential quizzes/assessments as Pre/Post proxies.
    \item \textbf{Downstream Performance:} Correlation between system usage/mastery and final course grades or standardized test scores.
\end{itemize}

\subsection{Engagement and Safety Metrics}
\begin{itemize}[noitemsep]
    \item \textbf{Engagement (Secondary):} Resource Consumption Rate (RCR), Click-Through Rate (CTR) for suggestions, Dwell Time. (Used as feature inputs, not success criteria).
    \item \textbf{Safety/Accuracy:} Hallucination Rate (vs. reference content), Contradiction Rate, Refusal/Deferral Accuracy (when the agent correctly refuses to answer or suggests external content).
\end{itemize}

\section{Stepwise Roadmap (12 Weeks)}

The plan is designed for shippable increments, allowing for A/B testing and decision points at the end of each phase.

\begin{longtable}{p{2.5cm}p{6.5cm}p{4.0cm}}
\caption{12-Week Adaptive Learning Roadmap} \\
\toprule
\textbf{Milestone} & \textbf{Key Tasks \& Focus} & \textbf{Acceptance Criteria \& Metrics} \\
\midrule
\endfirsthead
\caption{12-Week Adaptive Learning Roadmap (Cont.)} \\
\toprule
\textbf{Milestone} & \textbf{Key Tasks \& Focus} & \textbf{Acceptance Criteria \& Metrics} \\
\midrule
\endhead
\bottomrule
\endfoot
\endlastfoot

\textbf{M1 (Wks 1–2):} RAG Baseline \& Minimality Hard Caps &
1. Deploy basic RAG (Dense embedding + BM25) for retrieval.
2. Implement Content Minimization: Hard-cap all resource suggestions to 5 minutes/3 pages maximum.
3. Define chunking strategy and initial metadata schema.
4. Implement strict JSON-structured outputs for agent communication.
5. Offline Evaluation: Red team cases for irrelevant suggestions. &
1. nDCG@5 $\ge 0.65$ (offline evaluation using synthetic/SME labels).
2. Overkill Rate $\le 5\%$ (hard cap violations).
3. P95 Retrieval Latency $\le 1.5s$.
4. Decision Point: Proceed to advanced RAG or optimize embedding choice. \\

\textbf{M2 (Wks 3–4):} Advanced Retrieval \& Segmentation &
1. Implement Cross-Encoder Reranking on Top-50 candidates.
2. Integrate Video/PDF Segmenter (ASR for clips, section detection for PDF).
3. Introduce Sufficiency Score (semantic coverage / duration) ranking.
4. Set up telemetry logging for resource CTR and minimality metrics. &
1. nDCG@1 improves by $10\%$ vs. M1 baseline.
2. Median Resource Length $\le 3$ minutes.
3. First live A/B test (M2 RAG vs. M1 baseline) tracking CTR/RCR.
4. Decision Point: Verify data quality for resource segments. \\

\textbf{M3 (Wks 5–6):} Pedagogy Tools V1 \& Assessment Baseline &
1. Implement Prompted Question Generator (Q-Gen) grounded strictly in the suggested resource.
2. Develop Rubric Grader and Hinting features using few-shot exemplars from historical Q\&A.
3. Deploy initial IRT model (pre-calibrated with historical data) to track $\theta$.
4. Agentic Orchestration V1: Full Plan $\rightarrow$ Retrieve $\rightarrow$ Q-Gen $\rightarrow$ Assess loop operational. &
1. Question Quality Score (SME Rubric) $\ge 4.0/5.0$.
2. IRT Model calibration stability checked (SE for $\theta$ is low).
3. Live A/B test: M3 (full Q-Gen/Assessment) vs. M2 (no Q-Gen) tracking practice opportunities per session. \\

\textbf{M4 (Wks 7–8):} Bandit Optimization for Learning Gain &
1. Deploy Contextual Bandit (Thompson Sampling/UCB) policy server on top of M3 RAG.
2. Define and deploy Reward Service: $R = w_1(\text{Quiz Correctness Uplift}) + w_2(\text{Minimality})$.
3. Begin logging policy diagnostics and data for Off-Policy Evaluation (OPE).
4. Implement Safety/Guardrails: Source citation checks, JSON validation. &
1. Offline OPE demonstrates $\ge 5\%$ uplift potential in expected reward compared to RAG baseline.
2. Bandit Exploration Rate is calibrated (e.g., $10\%$).
3. Live A/B test: M4 Bandit Policy vs. M3 RAG Baseline, tracking $\Delta\theta$ per session. \\

\textbf{M5 (Wks 9–10):} LoRA Fine-Tuning \& Refinement &
1. Use high-quality labeled data (M1-M4 collection) to train LoRA adapters for Q-Gen and Distractor Quality (Consistency/Pedagogical Tone).
2. Implement adapter switching strategy for model upgrades.
3. Optimize Cross-Encoder for faster inference or replace with more compact architecture. &
1. Q-Gen Consistency (Style/Tone) improves by $20\%$ (measured by human/LLM evaluator).
2. Inference latency for Q-Gen tasks is reduced.
3. Live A/B test: Prompt-only Q-Gen vs. LoRA-tuned Q-Gen, tracking item quality and cost/latency. \\

\textbf{M6 (Wks 11–12):} Production Hardening \& Scaling &
1. Implement continuous monitoring, bias checks, and compliance filters (PII removal, accessibility).
2. Production-grade telemetry dashboards for all operational and learning metrics ($\theta$ drift, bandit diagnostics).
3. Finalize Educator Dashboard features for policy oversight and content performance. &
1. System uptime $\ge 99.9\%$ for core inference services.
2. Zero PII/Compliance violations found in final audit.
3. Final decision memo prepared on go/no-go for scaling based on M4/M5 $\Delta\theta$ results. \\
\end{longtable}

\section{Reinforcement Learning Design (Practical)}

The goal is to optimize the policy (the content selection engine) to maximize measurable learning outcomes while maintaining efficiency constraints.

\subsection{Reward Function Definition}
Since true learning gain ($\Delta\theta$) is a latent metric updated post-assessment, we define a composite, multi-objective reward ($R_t$) for selecting resource $r$ at time $t$:

$$R_t = w_1 \cdot \underbrace{\text{Quiz Correctness Uplift}}_{(\text{proxy for } \Delta\theta)} + w_2 \cdot \underbrace{\text{Minimality Bonus}}_{(\text{duration/pages})} - w_3 \cdot \underbrace{\text{Irrelevance Penalty}}_{(\text{low CTR/Skip})} - w_4 \cdot \underbrace{\text{Cost/Latency Penalty}}_{(\text{efficiency})}$$

\begin{itemize}[noitemsep]
    \item \textbf{Quiz Correctness Uplift ($\text{Proxy}$):} Score on the formative quiz immediately following the resource consumption. This serves as a rapid, high-frequency signal correlated with $\Delta\theta$. Weight $w_1$ is highest.
    \item \textbf{Minimality Bonus:} A scalar bonus inversely proportional to the segment duration, up to the hard cap.
    \item \textbf{Irrelevance Penalty:} Applied if the student skips the resource or fails the subsequent quiz significantly (suggesting irrelevance).
\end{itemize}

\subsection{Implementation Strategy: Contextual Bandits First}
\begin{enumerate}
    \item \textbf{Start with Contextual Bandits (M4):} Implement Thompson Sampling or UCB. Bandits are less data-hungry than full RL and excel at exploring the content selection space (Arms = Top-K minimal resources) based on immediate context (Learner $\theta$, question type, recent performance).
    \item \textbf{Context Features:} The context vector includes Learner $\theta$, Item parameters (difficulty of the current question topic), and features of the candidate resources (segment duration, source popularity, predicted relevance score from the Reranker).
    \item \textbf{Off-Policy Evaluation (OPE):} Essential for responsible iteration. Use Inverse Propensity Scoring (IPS) or Doubly Robust (DR) estimators to evaluate new policies (e.g., M5 Bandit Policy) offline using historical logged data before deployment, minimizing risk.
    \item \textbf{Escalation to Full RL:} Only transition to a full Reinforcement Learning setup (e.g., DQN or A2C) if the Contextual Bandit performance plateaus, and if we have successfully built a high-fidelity learning simulator (based on IRT parameter tracking and user behavior models) that allows for safe, large-scale training.
\end{enumerate}

\section{Fine-tuning Policy (When and How)}

The primary policy is to prioritize prompt engineering and RAG for core factual tasks, reserving fine-tuning for tasks requiring high consistency and style, ensuring agility.

\subsection{Task-Specific LoRA/PEFT Focus (M5)}
We will use LoRA (Low-Rank Adaptation) or similar PEFT methods on smaller, task-specific models (e.g., T5 or a smaller Gemini model) rather than full fine-tuning the primary large language model.

\begin{enumerate}
    \item \textbf{Pedagogical Consistency (Question Generation/Hinting):} Fine-tuning improves the consistency of question style, Bloom level alignment, and hint quality, reducing reliance on complex, long prompts.
    \item \textbf{Distractor Generation Quality:} Fine-tuning ensures distractors are plausible but incorrect, based on common learner misconceptions derived from historical data.
    \item \textbf{Rubric-Based Grading:} Fine-tuning to improve the accuracy of structured output grading against a complex internal rubric ontology.
\end{enumerate}

\subsection{Fine-Tuning Decision Checklist (Entry Criteria)}
Fine-tuning is approved only if ALL of these criteria are met for a specific task:
\begin{itemize}[noitemsep]
    \item \textbf{Performance Plateau:} Demonstrated inability of advanced prompt engineering (including Chain-of-Thought) to meet quality/consistency targets.
    \item \textbf{Data Availability:} $\ge 5,000$ high-quality, labeled exemplars per task (e.g., Question $\rightarrow$ LoRA $\rightarrow$ High-Quality Output).
    \item \textbf{Inference Savings/Latency:} Projected inference cost reduction or latency improvement justifies the training and serving overhead.
    \item \textbf{High Consistency Need:} The task requires absolute stylistic consistency (e.g., pedagogy tone, specific output format).
\end{itemize}

\subsection{Model Upgrade Migration Plan}
Using LoRA adapters ensures agility. If Google releases a new base model (e.g., Gemini 3), the migration strategy is:
\begin{enumerate}
    \item Test the performance of the new base model with existing prompt-only systems (M1-M4).
    \item Attempt adapter transfer: If the new base model is architecturally compatible, transfer the existing LoRA weights.
    \item Retrain only the small LoRA adapter weights (not the full model) on the new base model if transfer fails or performance degrades. This is significantly faster and cheaper than full fine-tuning.
\end{enumerate}

\section{Risks \& Mitigations}

\begin{longtable}{p{4.0cm}p{9.0cm}}
\caption{Key Risks and Mitigation Strategies} \\
\toprule
\textbf{Risk} & \textbf{Mitigation} \\
\midrule
\endfirsthead
\caption{Key Risks and Mitigation Strategies (Cont.)} \\
\toprule
\textbf{Risk} & \textbf{Mitigation} \\
\midrule
\endhead
\bottomrule
\endfoot
\endlastfoot

Cold-Start for content recommendations. & Implement heuristics and weak labeling (ASR + semantic coverage) immediately (M1/M2). Use SME review (Teacher-in-the-Loop) to generate initial high-quality labels for the Cross-Encoder. \\
Policy (Bandit) exploits simple metrics. & Use a multi-objective reward function with $w_1$ (Learning Gain proxy) as the dominant weight. Use IRT $\Delta\theta$ as the final, long-term truth metric, which is harder to game than simple correctness. \\
Over-long resources/Overkill. & Enforce strict hard caps (3-minute clips). Use the Sufficiency Score / Duration ratio as a hard ranking factor (M2). MMR ensures diverse coverage without excessive length. \\
Hallucinations (in Q-Gen or feedback). & Mandate Retrieval-Grounded Verification (RGV): Every generated question/hint must be traceable and verified against the suggested minimal resource content. Implement an answerability checker using the resource segment. \\
Misaligned Difficulty (IRT drift). & Establish a regular IRT recalibration cadence (e.g., monthly). Use anchor items (standard questions) to detect and correct parameter drift. Monitor item discrimination ($a$) as a quality filter. \\
Data Logging Volume/Privacy. & Implement strict PII filtering and anonymization at the logging layer (M6). Ensure role-based access to telemetry. Explore deployment options that maintain data within institutional boundaries. \\
\end{longtable}

\section{Deliverables per Milestone}

The following is a list of concrete, verifiable artifacts delivered at the completion of each milestone.

\begin{itemize}[noitemsep]
    \item \textbf{M1 (RAG Baseline):} Retrieval Pipeline Notebooks, Initial Embeddings and Vector DB Configuration, Hard-Cap Logic Code, Offline nDCG Evaluation Report, Red Team Stress Test Cases.
    \item \textbf{M2 (Advanced Reranking/Segmentation):} ASR/PDF Segmenter Pipeline, Cross-Encoder Training Data Card and Model Card, Sufficiency Score Implementation, Telemetry Dashboards V1 (CTR, Length).
    \item \textbf{M3 (Pedagogy/Assessment):} Q-Gen Prompt Library (version controlled), Rubric Grader Logic, IRT Model Artifacts (pre-calibrated parameters), Full Agentic Orchestration Flowchart, Live A/B Test Design Document.
    \item \textbf{M4 (Bandit Optimization):} Contextual Bandit Policy Server Code, Reward Service Implementation (including $w_1, w_2$ values), Off-Policy Evaluation (OPE) Recipe and Initial Report, Live $\Delta\theta$ Tracking Dashboard.
    \item \textbf{M5 (LoRA Fine-Tuning):} Labeled Dataset Card for Q-Gen FT, LoRA Adapter Weights and Model Card, Adapter Transfer/Migration Plan, Inference Latency Benchmark Report (FT vs. Prompt-Only).
    \item \textbf{M6 (Production Hardening):} Safety Guardrail Implementation (Contradiction checks, refusal paths), Compliance Audit Report (PII/Bias), Final Production Telemetry Dashboards, Decision Memo for Scaling.
\end{itemize}

\end{document}
```